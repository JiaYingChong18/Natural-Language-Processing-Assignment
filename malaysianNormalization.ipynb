{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "40a459be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: malaya in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (5.1.1)\n",
      "Requirement already satisfied: dateparser in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (1.2.1)\n",
      "Requirement already satisfied: scikit-learn>=1.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (1.6.1)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (2.32.3)\n",
      "Requirement already satisfied: unidecode in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (1.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (1.15.3)\n",
      "Requirement already satisfied: ftfy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (6.3.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (3.4.2)\n",
      "Requirement already satisfied: sentencepiece in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (0.2.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (4.67.1)\n",
      "Requirement already satisfied: malaya-boilerplate>=0.0.25 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (0.0.25)\n",
      "Requirement already satisfied: regex in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya) (2024.11.6)\n",
      "Requirement already satisfied: transformers in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from malaya) (4.52.4)\n",
      "Requirement already satisfied: huggingface-hub in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from malaya-boilerplate>=0.0.25->malaya) (0.32.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=1.2->malaya) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from scikit-learn>=1.2->malaya) (3.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from dateparser->malaya) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2024.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dateparser->malaya) (2025.2)\n",
      "Requirement already satisfied: tzlocal>=0.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from dateparser->malaya) (5.3.1)\n",
      "Requirement already satisfied: wcwidth in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from ftfy->malaya) (0.2.13)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->malaya) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->malaya) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->malaya) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->malaya) (2025.4.26)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tqdm->malaya) (0.4.6)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers->malaya) (3.18.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from transformers->malaya) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers->malaya) (6.0.2)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers->malaya) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from transformers->malaya) (0.5.3)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub->malaya-boilerplate>=0.0.25->malaya) (2025.5.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from huggingface-hub->malaya-boilerplate>=0.0.25->malaya) (4.13.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.7.0->dateparser->malaya) (1.17.0)\n",
      "Requirement already satisfied: tzdata in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tzlocal>=0.2->dateparser->malaya) (2025.2)\n",
      "Requirement already satisfied: fasttext-wheel in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (0.9.2)\n",
      "Requirement already satisfied: pybind11>=2.2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from fasttext-wheel) (2.13.6)\n",
      "Requirement already satisfied: setuptools>=0.7.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from fasttext-wheel) (75.8.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from fasttext-wheel) (2.1.3)\n",
      "Requirement already satisfied: pyenchant in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (3.2.2)\n",
      "Requirement already satisfied: tweepy in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (4.15.0)\n",
      "Requirement already satisfied: oauthlib<4,>=3.2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tweepy) (3.2.2)\n",
      "Requirement already satisfied: requests<3,>=2.27.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from tweepy) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib<3,>=1.2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from tweepy) (2.0.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27.0->tweepy) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27.0->tweepy) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27.0->tweepy) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests<3,>=2.27.0->tweepy) (2025.4.26)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install malaya --user\n",
    "!pip3 install fasttext-wheel --user\n",
    "!pip3 install pyenchant --user\n",
    "!pip install tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6ce817b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from nltk) (8.2.1)\n",
      "Requirement already satisfied: joblib in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (1.5.1)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Requirement already satisfied: emoji in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (2.14.1)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install nltk\n",
    "!pip3 install emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "9b91b998",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TweetTokenizer\n",
    "from emoji import demojize\n",
    "import re\n",
    "\n",
    "def cleanToken(token):\n",
    "    lowercase = token.lower()\n",
    "    if token.startswith(\"@\"):\n",
    "        return \"\"\n",
    "    elif lowercase.startswith(\"http\") or lowercase.startswith(\"www\"):\n",
    "        return \"\"\n",
    "    elif len(token) == 1:\n",
    "        return demojize(token)\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "def textPreprocess(text):\n",
    "    tokenizer = TweetTokenizer()\n",
    "    tokenized_tweet = tokenizer.tokenize(text.replace(\"’\", \"'\").replace(\"…\", \"...\"))\n",
    "    \n",
    "    normTweet = \" \".join([cleanToken(token) for token in tokenized_tweet])\n",
    "    #normTweet = [cleanToken(token) for token in tokenized_tweet if cleanToken(token)]\n",
    "    \n",
    "    normTweet = (\n",
    "        normTweet.replace(\"cannot \", \"can not \")\n",
    "        .replace(\"n't \", \" n't \")\n",
    "        .replace(\"n 't \", \" n't \")\n",
    "        .replace(\"ca n't\", \"can't\")\n",
    "        .replace(\"ai n't\", \"ain't\")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\"'m \", \" 'm \")\n",
    "        .replace(\"'re \", \" 're \")\n",
    "        .replace(\"'s \", \" 's \")\n",
    "        .replace(\"'ll \", \" 'll \")\n",
    "        .replace(\"'d \", \" 'd \")\n",
    "        .replace(\"'ve \", \" 've \")\n",
    "    )\n",
    "    normTweet = (\n",
    "        normTweet.replace(\" p . m .\", \"  p.m.\")\n",
    "        .replace(\" p . m \", \" p.m \")\n",
    "        .replace(\" a . m .\", \" a.m.\")\n",
    "        .replace(\" a . m \", \" a.m \")\n",
    "    )\n",
    "    \n",
    "    return normTweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "8bd472e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import malaya\n",
    "from typing import List\n",
    "def substring_rules(model, **kwargs):\n",
    "    \"\"\"\n",
    "    detect EN, MS and OTHER languages in a string.\n",
    "\n",
    "    EN words detection are using `pyenchant` from https://pyenchant.github.io/pyenchant/ and\n",
    "    user language detection model.\n",
    "\n",
    "    MS words detection are using `malaya.dictionary.is_malay` and\n",
    "    user language detection model.\n",
    "\n",
    "    OTHER words detection are using any language detection classification model, such as,\n",
    "    `malaya.language_detection.fasttext`.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : Callable\n",
    "        Callable model, must have `predict` method.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result : malaya.model.rules.LanguageDict class\n",
    "    \"\"\"\n",
    "\n",
    "fasttext = malaya.language_detection.fasttext()\n",
    "language_model = malaya.language_detection.substring_rules(model = fasttext)\n",
    "\n",
    "def predict(\n",
    "    self,\n",
    "    words: List[str],\n",
    "    acceptable_ms_label: List[str] = ['malay', 'ind'],\n",
    "    acceptable_en_label: List[str] = ['eng', 'manglish'],\n",
    "    use_is_malay: bool = True,\n",
    "):\n",
    "    \n",
    "\n",
    "    \"\"\"\n",
    "    Predict [EN, MS, OTHERS, CAPITAL, NOT_LANG] on word level.\n",
    "    This method assumed the string already tokenized.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    words: List[str]\n",
    "    acceptable_ms_label: List[str], optional (default = ['malay', 'ind'])\n",
    "        accept labels from language detection model to assume a word is `MS`.\n",
    "    acceptable_en_label: List[str], optional (default = ['eng', 'manglish'])\n",
    "        accept labels from language detection model to assume a word is `EN`.\n",
    "    use_is_malay: bool, optional (default=True)\n",
    "        if True`, will predict MS word using `malaya.dictionary.is_malay`,\n",
    "        else use language detection model.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    result: List[str]\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2f1cc0",
   "metadata": {},
   "source": [
    "# Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2d83d414",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:142: SyntaxWarning: invalid escape sequence '\\('\n",
      "<>:142: SyntaxWarning: invalid escape sequence '\\('\n",
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_29396\\267832134.py:142: SyntaxWarning: invalid escape sequence '\\('\n",
      "  \"(¬‿¬)\": \"smug\", \"(ʘ‿ʘ)\": \"creepy\", \"(☞ﾟヮﾟ)☞\": \"gotcha\", \"ヽ(´▽`)/\": \"cheerful\", \"\\(＾▽＾)/\":\"cheerful\"\n"
     ]
    }
   ],
   "source": [
    "#English dictionaries\n",
    "abbreviation_dict = {\n",
    "    \"afaik\": \"as far as I know\", \"afk\": \"away from keyboard\", \"asap\": \"as soon as possible\",\n",
    "    \"atm\": \"at the moment\", \"bbl\": \"be back later\", \"bfn\": \"bye for now\",\n",
    "    \"brb\": \"be right back\", \"btw\": \"by the way\", \"b/c\": \"because\", \"b4\": \"before\",\n",
    "    \"cu\": \"see you\", \"cya\": \"see you\", \"dm\": \"direct message\", \"fyi\": \"for your information\",\n",
    "    \"ftw\": \"for the win\", \"gg\": \"good game\", \"g2g\": \"got to go\", \"gtg\": \"got to go\",\n",
    "    \"gr8\": \"great\", \"hbu\": \"how about you\", \"hmu\": \"hit me up\", \"idc\": \"I don't care\",\n",
    "    \"idk\": \"I don't know\", \"ikr\": \"I know right\", \"ily\": \"I love you\", \"imho\": \"in my humble opinion\",\n",
    "    \"imo\": \"in my opinion\", \"irl\": \"in real life\", \"jk\": \"just kidding\", \"lmk\": \"let me know\",\n",
    "    \"lol\": \"laughing out loud\", \"lmao\": \"laughing my ass off\", \"lmfao\": \"laughing my freaking ass off\",\n",
    "    \"nvm\": \"never mind\", \"np\": \"no problem\", \"nsfw\": \"not safe for work\",\n",
    "    \"omg\": \"oh my god\", \"omw\": \"on my way\", \"otw\": \"on the way\", \"ppl\": \"people\",\n",
    "    \"pls\": \"please\", \"plz\": \"please\", \"rofl\": \"rolling on the floor laughing\", \"rn\": \"right now\",\n",
    "    \"smh\": \"shaking my head\", \"tbh\": \"to be honest\", \"tba\": \"to be announced\",\n",
    "    \"tbd\": \"to be decided\", \"thx\": \"thanks\", \"tho\": \"though\", \"ttyl\": \"talk to you later\",\n",
    "    \"tmi\": \"too much information\", \"tldr\": \"too long didn't read\", \"ty\": \"thank you\",\n",
    "    \"u\": \"you\", \"ur\": \"your\", \"wbu\": \"what about you\", \"wfh\": \"work from home\",\n",
    "    \"wth\": \"what the heck\", \"wtf\": \"what the f*\", \"wyd\": \"what are you doing\",\n",
    "    \"ya\": \"yeah\", \"yolo\": \"you only live once\", \"yw\": \"you're welcome\",\n",
    "    \"bff\": \"best friends forever\", \"faq\": \"frequently asked questions\", \"fb\": \"facebook\",\n",
    "    \"gm\": \"good morning\", \"gn\": \"good night\", \"grats\": \"congratulations\",\n",
    "    \"hbd\": \"happy birthday\", \"nm\": \"not much\", \"roflmao\": \"rolling on the floor laughing my ass off\",\n",
    "    \"sup\": \"what's up\", \"xoxo\": \"hugs and kisses\", \"zzz\": \"sleepy or bored\"\n",
    "}\n",
    "\n",
    "slang_dict = {\n",
    "    \"gonna\": \"going to\", \"wanna\": \"want to\", \"gotta\": \"got to\", \"kinda\": \"kind of\",\n",
    "    \"sorta\": \"sort of\", \"lemme\": \"let me\", \"gimme\": \"give me\", \"dunno\": \"do not know\",\n",
    "    \"coulda\": \"could have\", \"shoulda\": \"should have\", \"woulda\": \"would have\",\n",
    "    \"ain't\": \"is not\", \"can't\": \"cannot\", \"won't\": \"will not\", \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\", \"couldn't\": \"could not\", \"isn't\": \"is not\", \"aren't\": \"are not\",\n",
    "    \"wasn't\": \"was not\", \"weren't\": \"were not\", \"haven't\": \"have not\", \"hasn't\": \"has not\",\n",
    "    \"hadn't\": \"had not\", \"doesn't\": \"does not\", \"didn't\": \"did not\", \"don't\": \"do not\",\n",
    "    \"tryna\": \"trying to\", \"trynna\": \"trying to\", \"outta\": \"out of\", \"lotta\": \"a lot of\",\n",
    "    \"ain\": \"is not\", \"yo\": \"hey\", \"bro\": \"brother\", \"sis\": \"sister\", \"cuz\": \"because\",\n",
    "    \"bc\": \"because\", \"idk\": \"i do not know\", \"idc\": \"i do not care\", \"ily\": \"i love you\",\n",
    "    \"brb\": \"be right back\", \"bbl\": \"be back later\", \"bfn\": \"bye for now\", \"gtg\": \"got to go\",\n",
    "    \"ttyl\": \"talk to you later\", \"smh\": \"shaking my head\", \"tbh\": \"to be honest\",\n",
    "    \"imo\": \"in my opinion\", \"imho\": \"in my humble opinion\", \"ikr\": \"i know right\",\n",
    "    \"lmao\": \"laughing my ass off\", \"lol\": \"laugh out loud\", \"rofl\": \"rolling on the floor laughing\",\n",
    "    \"omg\": \"oh my god\", \"omw\": \"on my way\", \"bff\": \"best friend forever\",\n",
    "    \"wyd\": \"what are you doing\", \"wya\": \"where are you at\", \"wbu\": \"what about you\",\n",
    "    \"hbu\": \"how about you\", \"jk\": \"just kidding\", \"nvm\": \"never mind\", \"rn\": \"right now\",\n",
    "    \"tho\": \"though\", \"tldr\": \"too long did not read\", \"fyi\": \"for your information\",\n",
    "    \"np\": \"no problem\", \"ty\": \"thank you\", \"thx\": \"thanks\", \"yw\": \"you are welcome\",\n",
    "    \"u\": \"you\", \"ur\": \"your\", \"pls\": \"please\", \"plz\": \"please\", \"btw\": \"by the way\",\n",
    "    \"afaik\": \"as far as i know\", \"irl\": \"in real life\", \"dm\": \"direct message\",\n",
    "    \"fb\": \"facebook\", \"insta\": \"instagram\", \"snap\": \"snapchat\", \"tiktok\": \"tik tok\",\n",
    "    \"fomo\": \"fear of missing out\", \"yolo\": \"you only live once\", \"gr8\": \"great\",\n",
    "    \"bday\": \"birthday\", \"hbd\": \"happy birthday\", \"xoxo\": \"hugs and kisses\",\n",
    "    \"lmk\": \"let me know\", \"hmu\": \"hit me up\", \"gg\": \"good game\", \"sus\": \"suspicious\",\n",
    "    \"noob\": \"newbie\", \"rekt\": \"wrecked\", \"srsly\": \"seriously\", \"meh\": \"not interested\",\n",
    "    \"okie\": \"okay\", \"zzz\": \"sleepy\", \"ffs\": \"for fuck's sake\", \"wtf\": \"what the fuck\",\n",
    "    \"wth\": \"what the heck\", \"af\": \"as fuck\", \"broke af\": \"very broke\", \"lit\": \"exciting\",\n",
    "    \"slay\": \"perform excellently\", \"vibing\": \"enjoying the vibe\", \"lowkey\": \"not openly\",\n",
    "    \"highkey\": \"very obvious\", \"cap\": \"lie\", \"no cap\": \"not lying\", \"flex\": \"show off\",\n",
    "    \"clout\": \"internet fame\"\n",
    "}\n",
    "\n",
    "misspelling_dict = {\n",
    "    \"alr\": \"already\", \"alrd\": \"already\", \"alrdy\": \"already\",\n",
    "    \"abt\": \"about\", \"abtt\": \"about\", \"abou\": \"about\",\n",
    "    \"bcz\": \"because\", \"bcoz\": \"because\", \"bcuz\": \"because\", \"cuz\": \"because\",\n",
    "    \"bt\": \"but\", \"dat\": \"that\", \"dats\": \"that's\", \"dis\": \"this\",\n",
    "    \"frens\": \"friends\", \"fren\": \"friend\", \"gud\": \"good\",\n",
    "    \"gd\": \"good\", \"gr8\": \"great\", \"happi\": \"happy\", \"hpy\": \"happy\",\n",
    "    \"hv\": \"have\", \"jus\": \"just\", \"kno\": \"know\", \"knw\": \"know\",\n",
    "    \"laff\": \"laugh\", \"lmao\": \"laughing my ass off\", \"lolz\": \"laughing out loud\",\n",
    "    \"luv\": \"love\", \"lyk\": \"like\", \"nuthin\": \"nothing\",\n",
    "    \"nite\": \"night\", \"noe\": \"know\", \"nvm\": \"never mind\",\n",
    "    \"omg\": \"oh my god\", \"onli\": \"only\", \"oni\": \"only\",\n",
    "    \"oso\": \"also\", \"plz\": \"please\", \"pls\": \"please\",\n",
    "    \"prob\": \"problem\", \"rite\": \"right\", \"rly\": \"really\",\n",
    "    \"sry\": \"sorry\", \"srsly\": \"seriously\", \"shld\": \"should\",\n",
    "    \"smth\": \"something\", \"smh\": \"shaking my head\", \"smtg\": \"something\",\n",
    "    \"sum\": \"some\", \"sumone\": \"someone\", \"sumtimes\": \"sometimes\",\n",
    "    \"taht\": \"that\", \"teh\": \"the\", \"thx\": \"thanks\",\n",
    "    \"tilll\": \"till\", \"tmr\": \"tomorrow\", \"tmrw\": \"tomorrow\",\n",
    "    \"tonite\": \"tonight\", \"txt\": \"text\", \"ty\": \"thank you\",\n",
    "    \"u\": \"you\", \"ur\": \"your\", \"urself\": \"yourself\",\n",
    "    \"wanna\": \"want to\", \"gonna\": \"going to\", \"gotta\": \"got to\",\n",
    "    \"wat\": \"what\", \"wats\": \"what's\", \"wif\": \"with\",\n",
    "    \"wen\": \"when\", \"whr\": \"where\", \"whoz\": \"who is\",\n",
    "    \"wifey\": \"wife\", \"wld\": \"would\", \"wud\": \"would\",\n",
    "    \"woz\": \"was\", \"wuz\": \"was\", \"ya\": \"yeah\",\n",
    "    \"ytd\": \"yesterday\", \"yolo\": \"you only live once\", \"yea\": \"yeah\",\n",
    "    \"ya'll\": \"you all\", \"yall\": \"you all\", \"cya\": \"see you\",\n",
    "    \"bc\": \"because\", \"cme\": \"come\", \"cum\": \"come\",\n",
    "    \"dem\": \"them\", \"didnt\": \"didn't\", \"dont\": \"don't\",\n",
    "    \"doesnt\": \"doesn't\", \"couldnt\": \"couldn't\", \"wouldnt\": \"wouldn't\",\n",
    "    \"shouldnt\": \"shouldn't\", \"havent\": \"haven't\", \"hasnt\": \"hasn't\",\n",
    "    \"wasnt\": \"was not\", \"werent\": \"were not\", \"isnt\": \"is not\",\n",
    "    \"arnd\": \"around\", \"bday\": \"birthday\", \"hbd\": \"happy birthday\",\n",
    "    \"msg\": \"message\", \"bcoz\": \"because\", \"becoz\": \"because\"\n",
    "}\n",
    "\n",
    "expression_dict = {\n",
    "    \"ughh\": \"ugh\", \"ugh\": \"ugh\", \"uhh\": \"ugh\", \"omggg\": \"oh my god\", \"omg\": \"oh my god\",\n",
    "    \"huhh\": \"huh\", \"huh\": \"huh\", \"ehh\": \"eh\", \"ehhh\": \"eh\", \"eh\": \"\",\n",
    "    \"yeaah\": \"yeah\", \"yaa\": \"yeah\", \"yaaa\": \"yeah\", \"yaaaa\": \"yeah\", \"yeahhh\": \"yeah\",\n",
    "    \"nooo\": \"no\", \"noooo\": \"no\", \"nahh\": \"no\", \"nop\": \"nope\", \"nopeee\": \"nope\",\n",
    "    \"okk\": \"okay\", \"okkk\": \"okay\", \"okie\": \"okay\", \"okayy\": \"okay\", \"kay\": \"okay\",\n",
    "    \"haha\": \"laugh\", \"hahaha\": \"laugh\", \"ahahah\": \"laugh\", \"hehe\": \"laugh\", \"hehehe\": \"laugh\",\n",
    "    \"lol\": \"laughing out loud\", \"lolll\": \"laughing out loud\", \"lmao\": \"laughing my ass off\", \"rofl\": \"rolling on the floor laughing\", \"lmaoo\": \"laughing my ass off\",\n",
    "    \"wahhh\": \"wow\", \"woah\": \"wow\", \"woaah\": \"wow\", \"wowww\": \"wow\", \"wows\": \"wow\",\n",
    "    \"meh\": \"\", \"bleh\": \"\", \"blah\": \"\", \"tsk\": \"annoyed\", \"tsktsk\": \"annoyed\",\n",
    "    \"haiz\": \"sigh\", \"hais\": \"sigh\", \"sighhh\": \"sigh\", \"pfft\": \"dismissive\", \"pft\": \"dismissive\",\n",
    "    \"zzz\": \"bored or sleepy\", \"zzzz\": \"bored or sleepy\", \"zZz\": \"sleepy\", \"snore\": \"sleepy\", \"yawn\": \"tired\",\n",
    "    \"eeks\": \"scared\", \"eek\": \"scared\", \"eep\": \"scared\", \"gah\": \"frustration\", \"arggh\": \"frustration\",\n",
    "    \"grr\": \"anger\", \"grrr\": \"anger\", \"grrrr\": \"anger\", \"rawr\": \"roar\", \"boom\": \"explosion\",\n",
    "    \"bam\": \"impact\", \"brr\": \"cold\", \"brrr\": \"cold\", \"shh\": \"quiet\", \"shhh\": \"quiet\",\n",
    "    \"whoa\": \"wow\", \"omfgg\": \"oh my god\", \"damnn\": \"damn\", \"dayumm\": \"damn\", \"wtf\": \"what the fuck\",\n",
    "    \"wth\": \"what the heck\", \"smh\": \"shaking my head\", \"mehheh\": \"\", \"nya\": \"meow\", \"meoww\": \"meow\",\n",
    "    \"oof\": \"pain\", \"ouchh\": \"pain\", \"oww\": \"pain\", \"owww\": \"pain\", \"ughgh\": \"ugh\",\n",
    "    \"yay\": \"celebration\", \"yaay\": \"celebration\", \"yayyy\": \"celebration\", \"woohoo\": \"celebration\", \"hooray\": \"celebration\",\n",
    "    \"booo\": \"boo\", \"boohoo\": \"crying\", \"sob\": \"crying\", \"sniff\": \"crying\", \"sniffle\": \"crying\",\n",
    "    \"hmm\": \"thinking\", \"hmmm\": \"thinking\", \"hmmmm\": \"thinking\", \"mhm\": \"yes\", \"mmhmm\": \"yes\",\n",
    "    \"nah\": \"no\", \"yeet\": \"throw\", \"bruh\": \"bro\", \"yo\": \"hey\", \"heyya\": \"hey\",\n",
    "    \"sup\": \"what's up\", \"wassup\": \"what's up\", \"hiya\": \"hi\", \"helloo\": \"hello\", \"helooo\": \"hello\"\n",
    "}\n",
    "\n",
    "emoji_text_dict = {\n",
    "    \":)\": \"happy\", \":-)\": \"happy\", \":]\": \"happy\", \"=)\": \"happy\", \"^_^\": \"happy\",\n",
    "    \":(\": \"sad\", \":-(\": \"sad\", \":[\": \"sad\", \"=(\": \"sad\", \"T_T\": \"crying\",\n",
    "    \":D\": \"grin\", \":-D\": \"grin\", \"=D\": \"grin\", \"xD\": \"laughing\", \"XD\": \"laughing\",\n",
    "    \"xP\": \"playful\", \":P\": \"playful\", \":-P\": \"playful\", \";P\": \"playful\", \"=P\": \"playful\",\n",
    "    \";)\": \"wink\", \";-)\": \"wink\", \";]\": \"wink\", \"*_\": \"wink\", \"^-\": \"wink\",\n",
    "    \":O\": \"surprised\", \":-O\": \"surprised\", \":0\": \"surprised\", \"O_O\": \"surprised\", \"o_o\": \"surprised\",\n",
    "    \":'(\": \"crying\", \":'-(\": \"crying\", \";_;\": \"crying\", \"Q_Q\": \"crying\", \"T.T\": \"crying\",\n",
    "    \">:(\": \"angry\", \"D:<\": \"angry\", \">:[\": \"angry\", \">:O\": \"furious\", \"D8\": \"shocked\",\n",
    "    \":|\": \"neutral\", \":-|\": \"neutral\", \"--\": \"annoyed\", \"--\": \"annoyed\", \">>\": \"suspicious\",\n",
    "    \"<_<\": \"suspicious\", \">.<\": \"frustrated\", \">:O\": \"furious\", \"o_O\": \"confused\", \"O_o\": \"confused\",\n",
    "    \":/\": \"unsure\", \":-/\": \"unsure\", \":\\\\\": \"unsure\", \":-\\\\\": \"unsure\", \":S\": \"uneasy\",\n",
    "    \":-S\": \"uneasy\", \":3\": \"cute\", \"=3\": \"cute\", \"(^_^)\": \"shy\", \"uwu\": \"cute\",\n",
    "    \"owo\": \"curious\", \"^-^\": \"joyful\", \">w<\": \"excited\",  \"._.\": \"blank\",\n",
    "    \"-.-\": \"bored\", \"@_@\": \"dizzy\", \"x_x\": \"dead\", \"X_X\": \"dead\",\n",
    "    \">:D\": \"evil grin\", \">:3\": \"evil cute\", \"<3\": \"love\", \"</3\": \"broken heart\", \"♥\": \"love\",\n",
    "    \"^3^\": \"kiss\", \":\": \"kiss\", \":-\": \"kiss\", \":-X\": \"sealed lips\", \":X\": \"sealed lips\",\n",
    "    \">///<\": \"embarrassed\", \"^-^;\": \"nervous\", \"..;\": \"awkward\", \":$\": \"blushing\", \"(**)\": \"starstruck\",\n",
    "    \"(^^)\": \"smiling\", \"(><)\": \"upset\", \"(¬_¬)\": \"unimpressed\", \"(ಥ﹏ಥ)\": \"crying\", \"(╯°□°）╯︵ ┻━┻\": \"rage\",\n",
    "    \"(¬‿¬)\": \"smug\", \"(ʘ‿ʘ)\": \"creepy\", \"(☞ﾟヮﾟ)☞\": \"gotcha\", \"ヽ(´▽`)/\": \"cheerful\", \"\\(＾▽＾)/\":\"cheerful\"\n",
    "}\n",
    "\n",
    "\n",
    "PATTERN_CORRECTIONS = [\n",
    "    # Remove excessive repeated characters (but keep some for emphasis)\n",
    "    (r'(.)\\1{3,}', r'\\1\\1'),  # \"sooooo\" -> \"soo\"\n",
    "    # Handle common reduplication patterns\n",
    "    (r'(\\w+)2', r'\\1-\\1'),  # \"buat2\" -> \"buat-buat\"\n",
    "    # Clean up whitespace\n",
    "    (r'\\s+', ' '),\n",
    "    # Handle mixed case inconsistencies\n",
    "    (r'([a-z])([A-Z])', r'\\1 \\2'),\n",
    "    (r'\\n+', ' ')\n",
    "]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "6956450d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Malay dictionary\n",
    "\n",
    "import re\n",
    "import difflib\n",
    "from collections import Counter, defaultdict\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "abbreviation_dict_ms = {\n",
    "    # Original abbreviations\n",
    "    'sy': 'saya', 'x': 'tidak','xtau':'tidak tahu',  'aq': 'aku', 'kt': 'kita', 'tu': 'itu', 'ni': 'ini', 'yg': 'yang',\n",
    "    'dgn': 'dengan', 'dr': 'dari', 'dlm': 'dalam', 'tp': 'tetapi', 'tpi': 'tetapi',\n",
    "    'lg': 'lagi', 'utk': 'untuk', 'nk': 'nak', 'x': 'tak', 'xde': 'tiada',\n",
    "    'ade': 'ada', 'td': 'tadi', 'mlm': 'malam', 'ptg': 'petang', 'skrg': 'sekarang',\n",
    "    'skg': 'sekarang', 'b4': 'sebelum', 'btw': 'sebenarnya', 'lps': 'lepas',\n",
    "    'camtu': 'macam itu', 'camni': 'macam ini', 'shj': 'sahaja','je': 'sahaja', 'jgk': 'juga',\n",
    "    'plk': 'pula', 'bhai': 'kawan', 'minmin': 'admin', 'pstu': 'lepas tu',\n",
    "    'msk': 'masuk', 'kluar': 'keluar', 'gk': 'juga', 'blh': 'boleh', 'aku': 'saya','yg': 'yang',\n",
    "    'org': 'orang','kt': 'kita', 'tp':'tetapi',\n",
    "    'tpi': 'tetapi','tu': 'itu', 'ni': 'ini','je': 'sahaja', 'lah': '','je': 'sahaja','dgn': 'dengan',\n",
    "    'dr': 'dari','dlm': 'dalam', 'utk': 'untuk', 'nk': 'nak', 'xde': 'tiada', 'ade': 'ada', 'x': 'tak',             \n",
    "    'tak': 'tidak', 'doh': 'dah','sgt': 'sangat', 'byk': 'banyak', 'skrg': 'sekarang',\n",
    "    'skg': 'sekarang','mlm': 'malam','ptg': 'petang', 'td': 'tadi','klini': 'kali ini',\n",
    "    'kalini': 'kali ini',  'pstu': 'lepas tu', 'blh': 'boleh', 'b4': 'sebelum',\n",
    "    'lps': 'lepas','plk': 'pula', 'jc': 'jika',  'mmg': 'memang', 'sbb': 'sebab',\n",
    "    'sbbn': 'sebabnya','yg': 'yang', 'wkwk': 'ketawa', 'btw': 'sebenarnya',\n",
    "    \n",
    "    # Extended abbreviations\n",
    "    'kt': 'kita', 'kami': 'kami', 'awak': 'awak', 'dia': 'dia', 'dorg': 'mereka',\n",
    "    'ko': 'kamu', 'kau': 'kamu', 'hang': 'kamu', 'mu': 'kamu', 'lu': 'kamu',\n",
    "    'gue': 'aku', 'gw': 'aku', 'org': 'orang', 'urg': 'orang', 'urang': 'orang',\n",
    "    'brg': 'barang', 'bnd': 'benda', 'tmpat': 'tempat', 'msa': 'masa',\n",
    "    'hari': 'hari', 'hr': 'hari', 'thn': 'tahun', 'bln': 'bulan',\n",
    "    'mgg': 'minggu', 'jam': 'jam', 'mnit': 'minit', 'saat': 'saat',\n",
    "    'pagi': 'pagi', 'pg': 'pagi', 'tgh': 'tengah', 'tgh hari': 'tengah hari',\n",
    "    'siang': 'siang', 'sore': 'petang', 'subuh': 'subuh', 'isya': 'isya',\n",
    "    'mgr': 'manager', 'boss': 'bos', 'keje': 'kerja', 'krj': 'kerja',\n",
    "    'skolah': 'sekolah', 'skul': 'sekolah', 'uni': 'universiti', 'kolej': 'kolej',\n",
    "    'kls': 'kelas', 'exam': 'peperiksaan', 'quiz': 'kuiz', 'test': 'ujian',\n",
    "    'homework': 'kerja rumah', 'hw': 'kerja rumah', 'assignment': 'tugasan',\n",
    "    'proj': 'projek', 'presentation': 'pembentangan', 'meeting': 'mesyuarat',\n",
    "    'event': 'acara', 'party': 'parti', 'gathering': 'perhimpunan',\n",
    "    'family': 'keluarga', 'fam': 'keluarga', 'parents': 'ibu bapa',\n",
    "    'mama': 'ibu', 'papa': 'bapa', 'sis': 'kakak', 'bro': 'abang',\n",
    "    'adik': 'adik', 'kk': 'kakak', 'bg': 'abang', 'bb': 'baby',\n",
    "    'bf': 'teman lelaki', 'gf': 'teman perempuan', 'couple': 'pasangan',\n",
    "    'single': 'bujang', 'married': 'berkahwin', 'divorce': 'bercerai',\n",
    "    'food': 'makanan', 'mkn': 'makan', 'drink': 'minuman', 'minum': 'minum',\n",
    "    'breakfast': 'sarapan', 'lunch': 'makan tengah hari', 'dinner': 'makan malam',\n",
    "    'supper': 'makan lewat malam', 'snack': 'makanan ringan', 'dessert': 'pencuci mulut',\n",
    "    'house': 'rumah', 'home': 'rumah', 'office': 'pejabat', 'shop': 'kedai',\n",
    "    'mall': 'pusat membeli-belah', 'market': 'pasar', 'hospital': 'hospital',\n",
    "    'clinic': 'klinik', 'pharmacy': 'farmasi', 'bank': 'bank',\n",
    "    'car': 'kereta', 'bus': 'bas', 'train': 'kereta api', 'plane': 'kapal terbang',\n",
    "    'bike': 'basikal', 'motorcycle': 'motosikal', 'taxi': 'teksi',\n",
    "    'grab': 'grab', 'uber': 'uber', 'walking': 'berjalan kaki',\n",
    "    'money': 'duit', 'cash': 'tunai', 'credit card': 'kad kredit',\n",
    "    'debit card': 'kad debit', 'atm': 'atm', 'salary': 'gaji',\n",
    "    'bonus': 'bonus', 'saving': 'simpanan', 'loan': 'pinjaman',\n",
    "    'hp': 'telefon bimbit', 'phone': 'telefon', 'laptop': 'komputer riba',\n",
    "    'computer': 'komputer', 'tablet': 'tablet', 'internet': 'internet',\n",
    "    'wifi': 'wifi', 'social media': 'media sosial', 'facebook': 'facebook',\n",
    "    'instagram': 'instagram', 'twitter': 'twitter', 'whatsapp': 'whatsapp',\n",
    "    'game': 'permainan', 'movie': 'filem', 'music': 'muzik'\n",
    "    }\n",
    "\n",
    "slang_dict_ms = {\n",
    "    # Original slang\n",
    "    'lepak': 'bersantai', 'yumcha': 'minum-minum', 'mamak': 'kedai mamak',\n",
    "    'tapau': 'bungkus makanan', 'mcm': 'macam', 'giler': 'gila',\n",
    "    'best': 'seronok', 'bosan': 'tidak seronok', 'cun': 'cantik',\n",
    "    'terer': 'pandai', 'pelahap': 'makan gelojoh', 'hensem': 'kacak',\n",
    "    'jeles': 'cemburu', 'merajuk': 'rasa kecil hati', 'kecam': 'kritik teruk',\n",
    "    'berani': 'tidak takut', 'slowpoke': 'lambat', 'over': 'terlebih-lebih',\n",
    "    'susahh': 'susah', 'suke': 'suka', 'jom': 'mari',\n",
    "    \n",
    "    # Extended slang\n",
    "    'syok': 'seronok', 'shiok': 'seronok', 'steady': 'hebat', 'power': 'hebat',\n",
    "    'terror': 'pandai', 'terror ah': 'hebat sekali', 'geng': 'kumpulan kawan',\n",
    "    'bro': 'abang', 'sis': 'kakak', 'dude': 'kawan', 'mate': 'kawan',\n",
    "    'buddy': 'kawan', 'kaki': 'kawan rapat', 'gang': 'kumpulan',\n",
    "    'squad': 'kumpulan', 'crew': 'kumpulan', 'team': 'pasukan',\n",
    "    'oni': 'hanya', 'oso': 'juga', 'den': 'kemudian', 'den after': 'lepas tu',\n",
    "    'then': 'kemudian', 'after that': 'lepas tu', 'later': 'nanti',\n",
    "    'kawe': 'aku',  'aku': 'aku','sy': 'saya', 'aq': 'aku',\n",
    "    'weh': 'wei', 'woi': 'wei', 'wei': 'wei', 'bah': 'lah',             \n",
    "    'lah': '', 'leh': 'boleh', 'can': 'boleh', 'cannot': 'tak boleh',\n",
    "    'boleh': 'boleh', 'jangan': 'jangan', 'jom': 'mari', 'mcm': 'macam',\n",
    "    'mcm2': 'macam‐macam', 'gila': 'gila','giler': 'gila',\n",
    "    'best': 'terbaik','best2': 'terbaik','awesome': 'hebat',\n",
    "    'cool': 'hebat', 'cun': 'cantik', 'hensem': 'kacak',\n",
    "    'terer': 'pandai', 'sweet': 'manis','cemuih': 'muak',\n",
    "    'jeles': 'cemburu','merajuk': 'rasa kecil hati', 'pelahap': 'makan gelojoh',\n",
    "    'lepak': 'bersantai', 'yumcha': 'minum‐minum', 'mamak': 'kedai mamak',\n",
    "    'tapau': 'bungkus makanan',  'tawau': 'Tawau', 'kantoi': 'tertangkap',\n",
    "    'pedo': 'perancang', 'semborono': 'semborono','tempur': 'berlawan',\n",
    "    'bebel': 'membebel','resell': 'jual semula', 'socmed': 'media sosial',\n",
    "    'reti': 'tahu','gapo': 'apa', 'mikir': 'fikir',\n",
    "    'blaja': 'belajar', 'dokleh': 'tak boleh',  'otok': 'otak',\n",
    "    'slalu': 'selalu','jugak': 'juga',  'susahh': 'susah',\n",
    "    'germm': 'geram', 'over': 'terlebih‐lebih', 'bosan': 'tidak seronok',\n",
    "    'love': 'suka', 'sis': 'kakak','bro': 'abang','sis': 'kakak'\n",
    "    }\n",
    "\n",
    "misspelling_dict_ms = {\n",
    "    # Original misspellings\n",
    "    'aduiii': 'aduh', 'aduhh': 'aduh', 'haihhh': 'haih', 'peningg': 'pening',\n",
    "    'cantikk': 'cantik', 'bagusss': 'bagus', 'gerammm': 'geram',\n",
    "    'takkk': 'tak', 'sedihh': 'sedih', 'gembiraa': 'gembira',\n",
    "    'marahh': 'marah', 'bisingg': 'bising', 'jommm': 'jom',\n",
    "    'cepattt': 'cepat', 'sakittt': 'sakit', 'betull': 'betul',\n",
    "    'aduiii': 'aduh','aduhh': 'aduh', 'aduhhh': 'aduh',\n",
    "    'haihhh': 'haih','haih': 'haih', 'wuuiihhh': 'wow',       \n",
    "    'dimana': 'di mana','niemana': 'di mana', 'tuu': 'itu',\n",
    "    'tu': 'itu', 'tuuuu': 'itu', 'bulan2': 'bulan‐bulan',\n",
    "    'km': 'kamu', 'kau': 'kamu', 'kmu': 'kamu',  'dlg': 'dalang',\n",
    "    'pecaya': 'percaya', 'tulah': 'itulah','tula': 'itulah',\n",
    "    'byt': 'banyak', 'byk': 'banyak', 'skit': 'sakit',\n",
    "    'cantikk': 'cantik',  'cantikkk': 'cantik',  'bagusss': 'bagus',\n",
    "    'gerammm': 'geram', 'takkk': 'tak',  'sedihh': 'sedih',\n",
    "    'gembiraa': 'gembira',  'marahh': 'marah', 'bisingg': 'bising',\n",
    "    'jommm': 'jom', 'cepattt': 'cepat','sakittt': 'sakit',\n",
    "    'betull': 'betul','slalu': 'selalu', 'liat': 'lihat',\n",
    "    'liatt': 'lihat', 'sbg': 'sebagai','dgn': 'dengan',\n",
    "    'dlm': 'dalam','kmn': 'kemana','skrg': 'sekarang',\n",
    "    'skg': 'sekarang','klini': 'kali ini','kalini': 'kali ini',\n",
    "    'minmin': 'admin','Ai': 'saya','fwd': 'forward', 'snd': 'send',\n",
    "    \n",
    "    # Extended misspellings\n",
    "    'adoiii': 'aduh', 'alamakkk': 'alamak', 'wowww': 'wow', 'yeahhh': 'yeah',\n",
    "    'okayyy': 'okay', 'hmmm': 'hmm', 'errr': 'err', 'ummm': 'um',\n",
    "    'ahhhh': 'ah', 'ohhh': 'oh', 'ehhhh': 'eh', 'ishhhh': 'ish',\n",
    "    'cisss': 'cis', 'hadoiii': 'hadoi', 'cehhh': 'ceh', 'haishhhh': 'haish',\n",
    "    'aiyooo': 'aiyo', 'takpeee': 'takpe', 'lahhh': 'lah', 'lerrr': 'ler',\n",
    "    'mahhhh': 'mah', 'nehhhh': 'neh', 'lorrrr': 'lor', 'wahhhh': 'wah',\n",
    "    'haiiii': 'hai', 'byeee': 'bye', 'helloooo': 'hello', 'hiii': 'hi',\n",
    "    'thankssss': 'thanks', 'sorryyy': 'sorry', 'pleaseeee': 'please',\n",
    "    'helppp': 'help', 'waittt': 'wait', 'stopppp': 'stop', 'comeee': 'come',\n",
    "    'goooo': 'go', 'runnn': 'run', 'walkkkk': 'walk', 'sleeppp': 'sleep',\n",
    "    'eattt': 'eat', 'drinkkkk': 'drink', 'playyyy': 'play', 'workkkk': 'work',\n",
    "    'studyyy': 'study', 'readdd': 'read', 'writeee': 'write', 'speakkkk': 'speak',\n",
    "    'listennn': 'listen', 'lookkkk': 'look', 'seeee': 'see', 'hearrr': 'hear',\n",
    "    'smellll': 'smell', 'tasteee': 'taste', 'touchhhh': 'touch', 'feelll': 'feel',\n",
    "    'thinkkkk': 'think', 'knowww': 'know', 'understandddd': 'understand',\n",
    "    'remembeeer': 'remember', 'forgettt': 'forget', 'learnnnn': 'learn',\n",
    "    'teachhhh': 'teach', 'explainnn': 'explain', 'tellll': 'tell',\n",
    "    'askkkk': 'ask', 'answerrrr': 'answer', 'questionnn': 'question',\n",
    "    'talkkkk': 'talk', 'chattt': 'chat', 'discussss': 'discuss',\n",
    "    'argueee': 'argue', 'fighttt': 'fight', 'quarrelll': 'quarrel',\n",
    "    'peaceee': 'peace', 'loveee': 'love', 'hateee': 'hate',\n",
    "    'likeee': 'like', 'dislikeee': 'dislike', 'preferrr': 'prefer',\n",
    "    \n",
    "}\n",
    "\n",
    "expression_dict_ms = {\n",
    "    # Original expressions\n",
    "    'aduh': 'sakit atau terkejut', 'haih': 'keluhan', 'alamak': 'terkejut',\n",
    "    'ish': 'tidak setuju', 'eh': '', 'la': '', 'woi': 'marah atau panggilan',\n",
    "    'weh': 'panggilan atau hairan', 'cis': 'geram atau marah', \n",
    "    'hadoi': 'kesal', 'ceh': 'mengejek', 'haish': 'geram atau letih',\n",
    "    'aiyo': 'kesal atau bising', 'ehh': '', 'uhh': '', 'hmm': '',\n",
    "    'uhuk': 'batuk', 'takpe': 'tidak mengapa',\n",
    "    \n",
    "    # Extended expressions\n",
    "    'wah': 'kagum atau terkejut', 'wow': 'kagum', 'wahlau': 'terkejut atau marah',\n",
    "    'walao': 'terkejut atau marah', 'walao eh': 'sangat terkejut',\n",
    "    'aiya': 'kecewa atau kesal', 'alamak bhai': 'terkejut sekali',\n",
    "    'adoi': 'sakit atau penat', 'ado': 'sakit ringan', 'aduh mamak': 'sakit betul',\n",
    "    'ish ish': 'sangat tidak setuju', 'ish man': 'geram',\n",
    "    'cis bedebah': 'sangat marah', 'cis punya': 'marah',\n",
    "    'haish letih': 'sangat penat', 'haish bosan': 'sangat bosan',\n",
    "    'aiyo man': 'sangat kesal', 'aiyo boss': 'kesal dengan bos',\n",
    "    'eh macam mana': 'tanya cara', 'eh serious': 'tanya serius ke',\n",
    "    'eh betul ke': 'tanya betul ke', 'eh real or not': 'tanya betul ke',\n",
    "    'lah wei': 'panggilan kawan', 'lah dude': 'panggilan santai',\n",
    "    'mah dude': 'panggilan', 'neh bro': 'panggilan abang',\n",
    "    'lor sis': 'panggilan kakak', 'kan mate': 'panggilan kawan',\n",
    "    'la bhai': 'panggilan kawan india', 'la macha': 'panggilan kawan tamil',\n",
    "    'wei bro': 'panggilan abang', 'oi mate': 'panggilan kawan',\n",
    "    'yo dude': 'panggilan santai', 'sup bro': 'panggilan apa khabar',\n",
    "    'wassup': 'apa khabar', 'whatsup': 'apa khabar',\n",
    "    'howdy': 'apa khabar', 'g\\'day': 'selamat hari',\n",
    "    'morning': 'selamat pagi', 'evening': 'selamat petang',\n",
    "    'night': 'selamat malam', 'nite': 'selamat malam',\n",
    "    'goodnight': 'selamat malam', 'good morning': 'selamat pagi',\n",
    "    'good evening': 'selamat petang', 'good afternoon': 'selamat tengah hari',\n",
    "    'see ya': 'jumpa lagi', 'see you': 'jumpa lagi','aduh': 'aduh',\n",
    "    'aih': 'aiyo','aiyo': 'aiyo','alo': 'halo', 'alamak': 'alamak',        \n",
    "    'astagaaa': 'astaga','ais': 'ais','ish': 'ish',\n",
    "    'eh': '', 'em': '', 'hmm': '','uhh': '','uhuk': 'batuk',\n",
    "    'woi': 'woi','weh': 'weh', 'cis': 'geram','hadoi': 'kesal',\n",
    "    'ceh': 'ceh','haish': 'geram','hussh': 'diam','takpe': 'tak apa',\n",
    "    'tak apa': 'tak apa', 'ishk': 'ish','huhu': 'huhu',            \n",
    "    'lol': 'ketawa', 'lel': 'ketawa','haha': 'ketawa', 'hehe': 'ketawa kecil',\n",
    "    'haha': 'ketawa','hmm': '','ssst': 'diam'\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e181613",
   "metadata": {},
   "source": [
    "# Applying function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "e9e1aa7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified functions to work with individual tokens instead of sentences\n",
    "\n",
    "def apply_dictionary_cleaning_token(token, dict_mapping):\n",
    "    \"\"\"Apply dictionary replacements to a single token\"\"\"\n",
    "    if pd.isna(token) or token == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    token = str(token)\n",
    "    lower_token = token.lower()\n",
    "    \n",
    "    if lower_token in dict_mapping:\n",
    "        return dict_mapping[lower_token]\n",
    "    else:\n",
    "        return token\n",
    "\n",
    "def apply_pattern_corrections_token(token):\n",
    "    \"\"\"Apply pattern corrections to a single token\"\"\"\n",
    "    if pd.isna(token) or token == '':\n",
    "        return ''\n",
    "    \n",
    "    token = str(token)\n",
    "    for pattern, replacement in PATTERN_CORRECTIONS:\n",
    "        token = re.sub(pattern, replacement, token)\n",
    "    return token.strip()\n",
    "\n",
    "def text_cleaning_en_token(token):\n",
    "    \"\"\"Complete text cleaning pipeline for a single token\"\"\"\n",
    "    if pd.isna(token) or token == '':\n",
    "        return ''\n",
    "    \n",
    "   \n",
    "    token = str(token)\n",
    "    token = apply_pattern_corrections_token(token)\n",
    "    \n",
    "    # Apply dictionary cleanings in order\n",
    "    token = apply_dictionary_cleaning_token(token, abbreviation_dict)\n",
    "    token = apply_dictionary_cleaning_token(token, slang_dict)\n",
    "    token = apply_dictionary_cleaning_token(token, misspelling_dict)\n",
    "    token = apply_dictionary_cleaning_token(token, expression_dict)\n",
    "    token = apply_dictionary_cleaning_token(token, emoji_text_dict)\n",
    "    \n",
    "    return token\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "89735831",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modified functions to work with individual Malay tokens instead of sentences\n",
    "\n",
    "def apply_malay_dictionary_token(token):\n",
    "    \"\"\"\n",
    "    Apply Malay dictionaries to normalize a single token by replacing abbreviations, \n",
    "    slang, misspellings, and expressions with their standard forms.\n",
    "    \n",
    "    Args:\n",
    "        token: Single word/token to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned token or empty string if token should be removed\n",
    "    \"\"\"\n",
    "    if pd.isna(token) or token == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string and lowercase for matching\n",
    "    token = str(token)\n",
    "    original_token = token  # Keep original for punctuation preservation\n",
    "    token_lower = token.lower()\n",
    "    \n",
    "    # Clean token (remove punctuation for dictionary matching)\n",
    "    clean_token = re.sub(r'[^\\w]', '', token_lower)\n",
    "    \n",
    "    if not clean_token:  # If token is only punctuation\n",
    "        return original_token\n",
    "    \n",
    "    replacement = None\n",
    "    \n",
    "    # 1. Check abbreviations first\n",
    "    if clean_token in abbreviation_dict_ms:\n",
    "        replacement = abbreviation_dict_ms[clean_token]\n",
    "    \n",
    "    # 2. Check slang\n",
    "    elif clean_token in slang_dict_ms:\n",
    "        replacement = slang_dict_ms[clean_token]\n",
    "    \n",
    "    # 3. Check misspellings\n",
    "    elif clean_token in misspelling_dict_ms:\n",
    "        replacement = misspelling_dict_ms[clean_token]\n",
    "    \n",
    "    # 4. Check expressions\n",
    "    elif clean_token in expression_dict_ms:\n",
    "        replacement = expression_dict_ms[clean_token]\n",
    "    \n",
    "    # Handle replacement\n",
    "    if replacement is not None:\n",
    "        if replacement.strip():  # Non-empty replacement\n",
    "            return replacement\n",
    "        else:  # Empty replacement (token should be removed)\n",
    "            return ''\n",
    "    else:\n",
    "        return original_token  # Keep original token with punctuation\n",
    "\n",
    "def apply_pattern_corrections_token_ms(token):\n",
    "    \"\"\"Apply Malay pattern corrections to a single token\"\"\"\n",
    "    if pd.isna(token) or token == '':\n",
    "        return ''\n",
    "    \n",
    "    token = str(token)\n",
    "    for pattern, replacement in PATTERN_CORRECTIONS:\n",
    "        token = re.sub(pattern, replacement, token)\n",
    "    return token.strip()\n",
    "\n",
    "def text_cleaning_ms_token(token):\n",
    "    \"\"\"\n",
    "    Complete preprocessing pipeline for a single Malay token:\n",
    "    1. Apply pattern corrections\n",
    "    2. Apply Malay dictionary normalization\n",
    "    \n",
    "    Args:\n",
    "        token: Single word/token to be cleaned\n",
    "        \n",
    "    Returns:\n",
    "        Cleaned token\n",
    "    \"\"\"\n",
    "    if pd.isna(token) or token == '':\n",
    "        return ''\n",
    "    \n",
    "    # Convert to string if not already\n",
    "    token = str(token)\n",
    "    \n",
    "    # Apply pattern corrections\n",
    "    token = apply_pattern_corrections_token_ms(token)\n",
    "    \n",
    "    # Apply Malay dictionary normalization\n",
    "    cleaned_token = apply_malay_dictionary_token(token)\n",
    "    \n",
    "    return cleaned_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "00c6ad89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to process identified tokens with both English and Malay cleaning\n",
    "def process_identified_tokens_bilingual(identified_tokens):\n",
    "    \"\"\"\n",
    "    Process identified tokens with language-specific cleaning for both English and Malay\n",
    "    \n",
    "    Args:\n",
    "        identified_tokens: List of tuples (token, language_prediction)\n",
    "    \n",
    "    Returns:\n",
    "        List of cleaned tokens with their language predictions\n",
    "    \"\"\"\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    for token, lang_pred in identified_tokens:\n",
    "        cleaned_token = token\n",
    "        \n",
    "        # Apply English cleaning for English tokens\n",
    "        if lang_pred == 'EN':\n",
    "            cleaned_token = text_cleaning_en_token(token)\n",
    "        \n",
    "        # Apply Malay cleaning for Malay tokens\n",
    "        elif lang_pred == 'MS':\n",
    "            cleaned_token = text_cleaning_ms_token(token)\n",
    "        \n",
    "        # For OTHERS, try both cleanings\n",
    "        elif lang_pred == 'OTHERS':\n",
    "            # Try English cleaning first\n",
    "            english_cleaned = text_cleaning_en_token(token)\n",
    "            # Try Malay cleaning\n",
    "            malay_cleaned = text_cleaning_ms_token(token)\n",
    "            \n",
    "            # Choose the one that made a change, prioritize based on context\n",
    "            if english_cleaned != token and malay_cleaned != token:\n",
    "                cleaned_token = english_cleaned\n",
    "            elif english_cleaned != token:\n",
    "                cleaned_token = english_cleaned\n",
    "            elif malay_cleaned != token:\n",
    "                cleaned_token = malay_cleaned\n",
    "            else:\n",
    "                cleaned_token = token\n",
    "        \n",
    "        # Keep NOT_LANG and CAPITAL tokens as they are\n",
    "        elif lang_pred in ['NOT_LANG', 'CAPITAL']:\n",
    "            cleaned_token = token\n",
    "        \n",
    "        # Skip empty tokens (removed by Malay cleaning)\n",
    "        if cleaned_token.strip():\n",
    "            cleaned_tokens.append(cleaned_token)\n",
    "\n",
    "    return  ' '.join(cleaned_tokens)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83862092",
   "metadata": {},
   "source": [
    "# Post- processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7eb2c497",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\User\\anaconda3\\envs\\myenv\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# English Model\n",
    "punct_model_en = pipeline(\n",
    "    \"token-classification\",\n",
    "    model=\"oliverguhr/fullstop-punctuation-multilang-large\",\n",
    "    aggregation_strategy=\"simple\"\n",
    ")\n",
    "\n",
    "# Malay Model\n",
    "tokenizer_ms = AutoTokenizer.from_pretrained(\"mesolitica/finetune-true-case-t5-small-standard-bahasa-cased\")\n",
    "model_ms = AutoModelForSeq2SeqLM.from_pretrained(\"mesolitica/finetune-true-case-t5-small-standard-bahasa-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "de002138",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_en(text):\n",
    "\n",
    "    # Remove ALL @USER and HTTPURL patterns\n",
    "    text = re.sub(r'HTTPURL', '', text)  \n",
    "    text = re.sub(r'@USER[.,;:!?]*', '', text) \n",
    "    \n",
    "    # Clean extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Remove excessive punctuation \n",
    "    text = re.sub(r'([.!?])\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\.{2,}', '.', text) \n",
    "    text = re.sub(r'\\?{2,}', '?', text) \n",
    "    text = re.sub(r'!{2,}', '!', text) \n",
    "    \n",
    "    # Add punctuation\n",
    "    preds = punct_model_en(text)\n",
    "    insert_map = {}\n",
    "    \n",
    "    for p in preds:\n",
    "        label = p['entity_group']\n",
    "        punct = {\n",
    "            'PERIOD': '.',\n",
    "            'COMMA': ',',\n",
    "            'QUESTION': '?',\n",
    "            'EXCLAMATION': '!'\n",
    "        }.get(label, '')\n",
    "        if punct:\n",
    "            insert_map[p['end']] = punct\n",
    "    \n",
    "    result = []\n",
    "    last = 0\n",
    "    for idx in sorted(insert_map):\n",
    "        result.append(text[last:idx])\n",
    "        result.append(insert_map[idx])\n",
    "        last = idx\n",
    "    result.append(text[last:])\n",
    "    text = ''.join(result)\n",
    "    \n",
    "    # Clean spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Capitalize first letter of sentences\n",
    "    text = re.sub(r'(^|[.!?]\\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)\n",
    "        \n",
    "    # Ending punctuation\n",
    "    if text:\n",
    "        text = re.sub(r'[^\\w\\s.!?]*$', '', text)\n",
    "        text = re.sub(r'([.!?])[.!?]*$', r'\\1', text) \n",
    "        \n",
    "        if not text.endswith(('.', '!', '?')):\n",
    "            text += '.'\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "5628e1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess_ms(text):\n",
    "\n",
    "    # Remove ALL @USER and HTTPURL patterns\n",
    "    text = re.sub(r'HTTPURL', '', text)  \n",
    "    text = re.sub(r'@USER[.,;:!?]*', '', text) \n",
    "    \n",
    "    # Clean extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    if not text:\n",
    "        return text\n",
    "    \n",
    "    # Remove excessive punctuation \n",
    "    text = re.sub(r'([.!?])\\1{2,}', r'\\1', text)\n",
    "    text = re.sub(r'\\.{2,}', '.', text) \n",
    "    text = re.sub(r'\\?{2,}', '?', text) \n",
    "    text = re.sub(r'!{2,}', '!', text) \n",
    "    \n",
    "    # Add punctuation\n",
    "    inputs = tokenizer_ms(text, return_tensors=\"pt\", truncation=True)\n",
    "    with torch.no_grad():\n",
    "        output_ids = model_ms.generate(**inputs, max_length=512)\n",
    "    output = tokenizer_ms.decode(output_ids[0], skip_special_tokens=True)\n",
    "    \n",
    "    # Clean spacing around punctuation\n",
    "    text = re.sub(r'\\s+([.,!?])', r'\\1', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "    # Capitalize first letter of sentences\n",
    "    text = re.sub(r'(^|[.!?]\\s+)([a-z])', lambda m: m.group(1) + m.group(2).upper(), text)\n",
    "        \n",
    "    # Ending punctuation\n",
    "    if text:\n",
    "        text = re.sub(r'[^\\w\\s.!?]*$', '', text)\n",
    "        text = re.sub(r'([.!?])[.!?]*$', r'\\1', text) \n",
    "        \n",
    "        if not text.endswith(('.', '!', '?')):\n",
    "            text += '.'\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47a0632",
   "metadata": {},
   "source": [
    "# Test Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1deb9821",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mainFn(text, language):\n",
    "    testToken = textPreprocess(text)\n",
    "    tokenized_tweet = testToken.split()\n",
    "    identifiedTokens = list(zip(tokenized_tweet,language_model.predict(tokenized_tweet)))\n",
    "    print(testToken)\n",
    "    print(identifiedTokens)\n",
    "    cleanedText= process_identified_tokens_bilingual(identifiedTokens)\n",
    "    # identifiedTokens = list(zip(tokens, model.predict(tokens)))\n",
    "    # cleanedTokens, isMalayDominant = cleanNoise(identifiedTokens)\n",
    "    # cleanedText = \" \".join([word for word in cleanedTokens])\n",
    "    if (language == \"Malay\"):\n",
    "  #      print(\"After Malay text preprocessing: \"+ cleanedText)\n",
    "        translated = T5MalayaPredict(cleanedText)\n",
    "        return postprocess_ms(translated)\n",
    "    else:\n",
    "      #  cleanedText = text_cleaning_en(text)\n",
    "  #      print(\"After English text preprocessing: \"+ cleanedText)\n",
    "        translated = T5MalayaPredictEnglish(cleanedText)\n",
    "        return postprocess_en(translated)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "bea41366",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: RuthChg/fine_tuned_t5_preprocess_to_ms_2k_sample\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--RuthChg--fine_tuned_t5_preprocess_to_ms_2k_sample. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "output_model_path = \"RuthChg/fine_tuned_t5_preprocess_to_ms_2k_sample\"\n",
    "task_prefix = \"terjemah ke Melayu: \"\n",
    "print(f\"Loading model from: {output_model_path}\")\n",
    "    \n",
    "translator_pipeline = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=output_model_path,\n",
    "    tokenizer=output_model_path,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def T5MalayaPredict(text):\n",
    "    model_input = task_prefix + text\n",
    "    return (translator_pipeline(model_input, max_new_tokens=128)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "01538dfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from: RuthChg/fine_tuned_t5_preprocess_to_en_5k_sample\n",
      "English target\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Roaming\\Python\\Python312\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--RuthChg--fine_tuned_t5_preprocess_to_en_5k_sample. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Device set to use cpu\n"
     ]
    }
   ],
   "source": [
    "output_model_path2 = \"RuthChg/fine_tuned_t5_preprocess_to_en_5k_sample\"\n",
    "task_prefix2 = \"terjemah ke Inggeris: \"\n",
    "print(f\"Loading model from: {output_model_path2}\")\n",
    "print(\"English target\")\n",
    "    \n",
    "translator_pipeline2 = pipeline(\n",
    "    \"text2text-generation\",\n",
    "    model=output_model_path2,\n",
    "    tokenizer=output_model_path2,\n",
    "    device=0 if torch.cuda.is_available() else -1\n",
    ")\n",
    "\n",
    "def T5MalayaPredictEnglish(text):\n",
    "    model_input = task_prefix2 + text\n",
    "    return (translator_pipeline2(model_input, max_new_tokens=128)[0]['generated_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "d0601703",
   "metadata": {},
   "outputs": [],
   "source": [
    "#test = \"@JiayIng #bello Hi, I am JiaYing, harap u bila buat2 soooooooo ape shj xtau\"\n",
    "#testToken = textPreprocess(test)\n",
    "#print(testToken)\n",
    "#tokenized_tweet = testToken.split()\n",
    "#identifiedTokens = list(zip(tokenized_tweet,language_model.predict(tokenized_tweet)))\n",
    "#print(identifiedTokens)\n",
    "#process_identified_tokens_bilingual(identifiedTokens)\n",
    "#print(\"After text preprocessing: \" + process_identified_tokens_bilingual(identifiedTokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4251ae9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d4f4b77b",
   "metadata": {},
   "source": [
    "# Interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "55eae09f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (5.31.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (4.9.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.115.12)\n",
      "Requirement already satisfied: ffmpy in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.5.0)\n",
      "Requirement already satisfied: gradio-client==1.10.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (1.10.1)\n",
      "Requirement already satisfied: groovy~=0.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx>=0.24.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.28.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (0.32.2)\n",
      "Requirement already satisfied: jinja2<4.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (3.0.2)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (2.1.3)\n",
      "Requirement already satisfied: orjson~=3.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (3.10.18)\n",
      "Requirement already satisfied: packaging in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (25.0)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (2.2.3)\n",
      "Requirement already satisfied: pillow<12.0,>=8.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (11.2.1)\n",
      "Requirement already satisfied: pydantic<2.12,>=2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (2.11.5)\n",
      "Requirement already satisfied: pydub in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from gradio) (6.0.2)\n",
      "Requirement already satisfied: ruff>=0.9.3 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.11.11)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.6 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.1.6)\n",
      "Requirement already satisfied: semantic-version~=2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.46.2)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.13.2)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.16.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (4.13.2)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio) (0.34.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio-client==1.10.1->gradio) (2025.5.1)\n",
      "Requirement already satisfied: websockets<16.0,>=10.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from gradio-client==1.10.1->gradio) (15.0.1)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
      "Requirement already satisfied: certifi in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from httpx>=0.24.1->gradio) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpx>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\n",
      "Requirement already satisfied: requests in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from click>=8.0.0->typer<1.0,>=0.12->gradio) (0.4.6)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\user\\appdata\\roaming\\python\\python312\\site-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.4.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\user\\anaconda3\\envs\\myenv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
     ]
    }
   ],
   "source": [
    "!pip3 install gradio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c267b52a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aku ingat kan bila dah pakai ip ni gambar tu lawa lah estetek lah jadi tapi lupa diri sendiri tak reti ambik gambarâ € ¦ \n",
      "[('Aku', 'MS'), ('ingat', 'MS'), ('kan', 'MS'), ('bila', 'MS'), ('dah', 'MS'), ('pakai', 'MS'), ('ip', 'OTHERS'), ('ni', 'MS'), ('gambar', 'MS'), ('tu', 'MS'), ('lawa', 'MS'), ('lah', 'MS'), ('estetek', 'OTHERS'), ('lah', 'MS'), ('jadi', 'MS'), ('tapi', 'MS'), ('lupa', 'MS'), ('diri', 'MS'), ('sendiri', 'MS'), ('tak', 'MS'), ('reti', 'MS'), ('ambik', 'MS'), ('gambarâ', 'OTHERS'), ('€', 'OTHERS'), ('¦', 'OTHERS')]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Aku ingat kan bila dah pakai ip ni gambar tu lawa lah estetek lah jadi tapi lupa diri sendiri tak reti ambik gambarâ € ¦ \n",
      "[('Aku', 'MS'), ('ingat', 'MS'), ('kan', 'MS'), ('bila', 'MS'), ('dah', 'MS'), ('pakai', 'MS'), ('ip', 'OTHERS'), ('ni', 'MS'), ('gambar', 'MS'), ('tu', 'MS'), ('lawa', 'MS'), ('lah', 'MS'), ('estetek', 'OTHERS'), ('lah', 'MS'), ('jadi', 'MS'), ('tapi', 'MS'), ('lupa', 'MS'), ('diri', 'MS'), ('sendiri', 'MS'), ('tak', 'MS'), ('reti', 'MS'), ('ambik', 'MS'), ('gambarâ', 'OTHERS'), ('€', 'OTHERS'), ('¦', 'OTHERS')]\n",
      " Aku ingat kan bila dah pakai ip ni gambar tu lawa lah estetek lah jadi tapi lupa diri sendiri tak reti ambik gambarâ € ¦ \n",
      "[('Aku', 'MS'), ('ingat', 'MS'), ('kan', 'MS'), ('bila', 'MS'), ('dah', 'MS'), ('pakai', 'MS'), ('ip', 'OTHERS'), ('ni', 'MS'), ('gambar', 'MS'), ('tu', 'MS'), ('lawa', 'MS'), ('lah', 'MS'), ('estetek', 'OTHERS'), ('lah', 'MS'), ('jadi', 'MS'), ('tapi', 'MS'), ('lupa', 'MS'), ('diri', 'MS'), ('sendiri', 'MS'), ('tak', 'MS'), ('reti', 'MS'), ('ambik', 'MS'), ('gambarâ', 'OTHERS'), ('€', 'OTHERS'), ('¦', 'OTHERS')]\n"
     ]
    }
   ],
   "source": [
    "import gradio as gr\n",
    "\n",
    "with gr.Blocks() as demo:\n",
    "    title = gr.HTML(\"<h1>Simple Malaysian Tweet Normalizer</h1>\")\n",
    "    description =  gr.HTML(\"<h3>Converts your Malaysian Tweet text into standardized language!</h3>\")\n",
    "    \n",
    "    text = gr.Textbox(lines=3, placeholder=\"Enter tweet text here...\", label=\"Raw tweet text\")\n",
    "    language = gr.Radio([\"English\", \"Malay\"], label=\"Choose your target language to translate to: \")\n",
    "    outputs = gr.Textbox(label=\"Normalized text\", lines=3, interactive=False),\n",
    "\n",
    "    gr.Button(\"Translate\").click(\n",
    "        fn=mainFn,\n",
    "        inputs = [text, language],\n",
    "        outputs = outputs\n",
    "    )\n",
    "\n",
    "def intFn(text, language):\n",
    "    mainFn(text, language)\n",
    "\n",
    "demo.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
